{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 2\n",
    "## Finding best policies in simple MDPs\n",
    "\n",
    "Great work making the MDPs in Week 1!\n",
    "\n",
    "In this assignment, we'll use the simplest RL techniques - Policy and Value iteration to find the best policies (which maximize the discounted total reward) in our MDPs from last week.\n",
    "\n",
    "Feel free to use your own MDPs, or import them from the OpenAI Gym library.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "Let's now try to solve the Frozen Lake environment for some cases\n",
    "\n",
    "To align with Grokking, let us consider an unusual $\\gamma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 is to import stuff\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 is to get the MDP\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "env = env.unwrapped\n",
    "mdp_transitions = env.P\n",
    "init_state = env.reset()\n",
    "goal_state = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 is to write the policy\n",
    "\n",
    "# This is according to the convention of gymnasium\n",
    "LEFT, DOWN, RIGHT, UP = range(4)\n",
    "pi = {\n",
    "    0:RIGHT, 1:RIGHT, 2:DOWN, 3:LEFT,\n",
    "    4:DOWN, 5:LEFT, 6:DOWN, 7:LEFT,\n",
    "    8:RIGHT, 9:RIGHT, 10:DOWN, 11:LEFT,\n",
    "    12:LEFT, 13:RIGHT, 14:RIGHT, 15:LEFT\n",
    "}\n",
    "\n",
    "# Or you can do it randomly\n",
    "# pi = dict()\n",
    "# for state in mdp:\n",
    "#     pi[state] = np.random.choice(mdp[state].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 is computing the value function for this envi and policy\n",
    "\n",
    "# Let us start with a random value function\n",
    "\n",
    "val = dict()\n",
    "for state in mdp_transitions:\n",
    "    val[state] = np.random.random()\n",
    "\n",
    "# Since 5, 7, 11, 12 and 15 are terminal states, we know their values are 0\n",
    "\n",
    "val[5] = 0\n",
    "val[7] = 0\n",
    "val[11] = 0\n",
    "val[12] = 0\n",
    "val[15] = 0\n",
    "\n",
    "#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using \n",
    "# val = dict()\n",
    "# for state in mdp:\n",
    "#     val[state] = np.random.random()\n",
    "#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state\n",
    "#         val[state] = 0\n",
    "\n",
    "#instead of doing thsi you can simply intialize the value function to 0 for all states \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_value_fn(val, mdp, pi, gamma = 1.0):\n",
    "    new_val = dict()\n",
    "    state_new_val = 0\n",
    "    for state in pi:\n",
    "        state_new_val = 0\n",
    "        action = pi[state]\n",
    "        for my_tupple in mdp[state][action]:\n",
    "            prob = my_tupple[0]\n",
    "            next_state = my_tupple[1]\n",
    "            new_reward = my_tupple[2]\n",
    "            done = my_tupple[3]\n",
    "            state_new_val += prob*(new_reward * (not done) + gamma * val[next_state])\n",
    "        new_val[state] = state_new_val  \n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[729], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val, count \n\u001b[1;32m---> 19\u001b[0m \u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmdp_transitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1e-7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[729], line 7\u001b[0m, in \u001b[0;36mpolicy_evaluation\u001b[1;34m(val, mdp, pi, epsilon, gamma)\u001b[0m\n\u001b[0;32m      5\u001b[0m old_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(val\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m      6\u001b[0m new_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(get_new_value_fn(val,mdp,pi,gamma)\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m----> 7\u001b[0m d1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m difference \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(d1)\n\u001b[0;32m      9\u001b[0m max_element \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(difference)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Use to above function to get the new value function, also print how many iterations it took to converge\n",
    "def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):\n",
    "    count = 0\n",
    "    while True:\n",
    "        old_value = np.array(list(val.values()))\n",
    "        new_value = np.array(list(get_new_value_fn(val,mdp,pi,gamma).values()))\n",
    "        d1 = np.subtract(new_value, old_value)\n",
    "        difference = np.abs(d1)\n",
    "        max_element = np.max(difference)\n",
    "        if max_element < epsilon:\n",
    "            val = new_value\n",
    "            count +=1\n",
    "            break\n",
    "        else:\n",
    "            old_value = new_value\n",
    "            count += 1\n",
    "\n",
    "    return val, count \n",
    "policy_evaluation(val,mdp_transitions,pi,1e-7,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary\n",
    "def policy_improvement(val, mdp, pi, gamma=1.0):\n",
    "    new_pi = dict()\n",
    "    q = dict()\n",
    "    # Complete this function to get the new policy given the value function and the mdp\n",
    "    return new_pi, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge\n",
    "# Create a random policy and value function to start with or use the ones defined above\n",
    "def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):\n",
    "    pi = dict()\n",
    "    val = dict()\n",
    "    count = 0\n",
    "    # Complete this function to get the optimal policy and value function and return the total number of iterations it took to converge\n",
    "    return pi, val, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge\n",
    "def value_iteration(mdp, gamma=1.0, epsilon=1e-10):\n",
    "    val = {s: 0 for s in mdp}\n",
    "    count = 0\n",
    "    q = dict()\n",
    "    # Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge\n",
    "    pi = {s: max(q[s], key=q[s].get) for s in mdp}\n",
    "    return pi, val, count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))\n",
    "env2 = env2.unwrapped\n",
    "mdp2 = env2.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[723], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pi1, val1, count1 \u001b[38;5;241m=\u001b[39m policy_iteration(mdp2)\n\u001b[1;32m----> 2\u001b[0m pi2, val2, count2 \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdp2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[721], line 7\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m(mdp, gamma, epsilon)\u001b[0m\n\u001b[0;32m      5\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Complete this function to get the optimal policy, optimal value function and return the total number of iterations it took to converge\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pi \u001b[38;5;241m=\u001b[39m {s: \u001b[38;5;28mmax\u001b[39m(\u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m, key\u001b[38;5;241m=\u001b[39mq[s]\u001b[38;5;241m.\u001b[39mget) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m mdp}\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pi, val, count\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "pi1, val1, count1 = policy_iteration(mdp2)\n",
    "pi2, val2, count2 = value_iteration(mdp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write a function `test_policy()` to test your policy after training to find the number of times you reached the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(pi, env, goalstate):\n",
    "    # Complete this function to test the policy\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
