{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 2\n",
    "## Finding best policies in simple MDPs\n",
    "\n",
    "Great work making the MDPs in Week 1!\n",
    "\n",
    "In this assignment, we'll use the simplest RL techniques - Policy and Value iteration to find the best policies (which maximize the discounted total reward) in our MDPs from last week.\n",
    "\n",
    "Feel free to use your own MDPs, or import them from the OpenAI Gym library.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-3.\n",
    "\n",
    "For this you have to install gymnasium, which is an API standard for reinforcement learning with a diverse collection of reference environments. This can be easily done by running:\n",
    "\n",
    "    pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "\n",
    "Let's now try to solve the Frozen Lake environment for some cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 is to import stuff\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)]},\n",
      " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)]},\n",
      " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 6, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 6, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 6, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)]},\n",
      " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False)]},\n",
      " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     2: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)]},\n",
      " 5: {0: [(1.0, 5, 0, True)],\n",
      "     1: [(1.0, 5, 0, True)],\n",
      "     2: [(1.0, 5, 0, True)],\n",
      "     3: [(1.0, 5, 0, True)]},\n",
      " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 10, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True)],\n",
      "     2: [(0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)]},\n",
      " 7: {0: [(1.0, 7, 0, True)],\n",
      "     1: [(1.0, 7, 0, True)],\n",
      "     2: [(1.0, 7, 0, True)],\n",
      "     3: [(1.0, 7, 0, True)]},\n",
      " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 12, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 12, 0.0, True),\n",
      "         (0.3333333333333333, 9, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 12, 0.0, True),\n",
      "         (0.3333333333333333, 9, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 9, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False)]},\n",
      " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 13, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 13, 0.0, False),\n",
      "         (0.3333333333333333, 10, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 13, 0.0, False),\n",
      "         (0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     3: [(0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 8, 0.0, False)]},\n",
      " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 11, 0.0, True)],\n",
      "      2: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 11, 0.0, True),\n",
      "          (0.3333333333333333, 6, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 11, 0.0, True),\n",
      "          (0.3333333333333333, 6, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False)]},\n",
      " 11: {0: [(1.0, 11, 0, True)],\n",
      "      1: [(1.0, 11, 0, True)],\n",
      "      2: [(1.0, 11, 0, True)],\n",
      "      3: [(1.0, 11, 0, True)]},\n",
      " 12: {0: [(1.0, 12, 0, True)],\n",
      "      1: [(1.0, 12, 0, True)],\n",
      "      2: [(1.0, 12, 0, True)],\n",
      "      3: [(1.0, 12, 0, True)]},\n",
      " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 12, 0.0, True),\n",
      "          (0.3333333333333333, 13, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 12, 0.0, True),\n",
      "          (0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      2: [(0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 12, 0.0, True)]},\n",
      " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 15, 1.0, True)],\n",
      "      2: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 15, 1.0, True),\n",
      "          (0.3333333333333333, 10, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 15, 1.0, True),\n",
      "          (0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 13, 0.0, False)]},\n",
      " 15: {0: [(1.0, 15, 0, True)],\n",
      "      1: [(1.0, 15, 0, True)],\n",
      "      2: [(1.0, 15, 0, True)],\n",
      "      3: [(1.0, 15, 0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1 is to get the MDP\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "env = env.unwrapped\n",
    "mdp_transitions = env.P\n",
    "init_state = env.reset()\n",
    "goal_state = 15\n",
    "\n",
    "pprint.pprint(mdp_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 is to write the policy\n",
    "\n",
    "# This is according to the convention of gymnasium\n",
    "LEFT, DOWN, RIGHT, UP = range(4)\n",
    "\n",
    "pi = {\n",
    "    0:RIGHT, 1:RIGHT, 2:DOWN, 3:LEFT,\n",
    "    4:DOWN, 5:LEFT, 6:DOWN, 7:LEFT,\n",
    "    8:RIGHT, 9:RIGHT, 10:DOWN, 11:LEFT,\n",
    "    12:LEFT, 13:RIGHT, 14:RIGHT, 15:LEFT\n",
    "}\n",
    "\n",
    "# Or you can do it randomly\n",
    "# pi = dict()\n",
    "# for state in mdp:\n",
    "#     pi[state] = np.random.choice(mdp[state].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 is computing the value function for this envi and policy\n",
    "\n",
    "# Let us start with a random value function\n",
    "\n",
    "val = dict()\n",
    "for state in mdp_transitions:\n",
    "    val[state] = 0\n",
    "\n",
    "Holes = [5,7,11,12,15]\n",
    "# Since 5, 7, 11, 12 and 15 are terminal states, we know their values are 0\n",
    "\n",
    "#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using \n",
    "# val = dict()\n",
    "# for state in mdp:\n",
    "#     val[state] = np.random.random()\n",
    "#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state\n",
    "#         val[state] = 0\n",
    "\n",
    "#instead of doing this you can simply intialize the value function to 0 for all states \n",
    "# for state in swf_mdp:\n",
    "#   val[state] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_value_fn(val, mdp, pi, gamma = 1.0):\n",
    "    new_val = dict()\n",
    "    for state in range(16):\n",
    "        if state in Holes:\n",
    "            new_val[state]=0\n",
    "        else:\n",
    "            state_new_val = 0\n",
    "            action = pi[state]\n",
    "            for my_tupple in mdp[state][action]:\n",
    "                prob,next_state,new_reward,done = my_tupple\n",
    "                state_new_val += prob*(new_reward + gamma * val[next_state])\n",
    "            new_val[state] = state_new_val          \n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):\n",
    "    count = 0\n",
    "    start_v = val\n",
    "    max_difference = 1\n",
    "    while max_difference>epsilon:\n",
    "        prev_v = start_v\n",
    "        next_v = get_new_value_fn(prev_v,mdp,pi,gamma)\n",
    "        keys = sorted(prev_v.keys())\n",
    "        prev_values = [prev_v[key] for key in keys]\n",
    "        next_values = [next_v[key] for key in keys]\n",
    "        differences = [abs(next_val - prev_val) for next_val, prev_val in zip(next_values, prev_values)]\n",
    "        max_difference = max(differences)\n",
    "        count +=1\n",
    "        start_v = next_v \n",
    "    return next_v,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(val, mdp, pi, gamma=1.0):\n",
    "    new_pi = dict()\n",
    "    q = dict()\n",
    "    for state in range(16):\n",
    "        empty_q_1 = {0:0, 1:0, 2:0, 3:0}\n",
    "        q[state] = empty_q_1\n",
    "    for state in range(len(mdp)):\n",
    "        actions_dict = mdp[state]\n",
    "        for action in range(len(mdp[0])):\n",
    "            action_list = actions_dict[action]\n",
    "            q_value = 0\n",
    "            for _ in action_list:\n",
    "                prob,next_state,new_reward,done = _\n",
    "                q_value += prob*(new_reward + gamma * val[next_state]*(not done))\n",
    "            q[state][action] = q_value  \n",
    "    list_actions = [0,1,2,3]    \n",
    "    for state in q:\n",
    "        temp_dict = q[state]\n",
    "        max_key = max(temp_dict, key=temp_dict.get)\n",
    "        new_pi[state] = list_actions[max_key]\n",
    "    return new_pi, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 0,\n",
       "  1: 3,\n",
       "  2: 3,\n",
       "  3: 3,\n",
       "  4: 0,\n",
       "  5: 0,\n",
       "  6: 0,\n",
       "  7: 0,\n",
       "  8: 3,\n",
       "  9: 1,\n",
       "  10: 0,\n",
       "  11: 0,\n",
       "  12: 0,\n",
       "  13: 2,\n",
       "  14: 1,\n",
       "  15: 0},\n",
       " ({0: 0.8235294095045046,\n",
       "   1: 0.8235294087468735,\n",
       "   2: 0.823529408208908,\n",
       "   3: 0.8235294079297658,\n",
       "   4: 0.8235294096690228,\n",
       "   5: 0,\n",
       "   6: 0.5294117630897942,\n",
       "   7: 0,\n",
       "   8: 0.823529409986084,\n",
       "   9: 0.8235294104326095,\n",
       "   10: 0.7647058811781084,\n",
       "   11: 0,\n",
       "   12: 0,\n",
       "   13: 0.8823529402305985,\n",
       "   14: 0.9411764700974368,\n",
       "   15: 0},\n",
       "  1),\n",
       " 4)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):\n",
    "    pi = dict()\n",
    "    val = {s: 0 for s in mdp}\n",
    "    count = 0\n",
    "    old_pi = {s: random.randint(0,3) for s in mdp} #starting with a random policy\n",
    "    temp_val,_ = policy_evaluation(val,mdp,old_pi,epsilon,gamma)\n",
    "    while True:\n",
    "        new_val,_ = policy_evaluation(temp_val, mdp, old_pi, epsilon, gamma)\n",
    "        new_pi,_ = policy_improvement(new_val, mdp, old_pi, gamma)\n",
    "        count += 1\n",
    "        if new_pi == old_pi:\n",
    "            pi = new_pi\n",
    "            val = policy_evaluation(new_val, mdp, new_pi, epsilon, gamma)\n",
    "            break\n",
    "        else:\n",
    "            old_pi = new_pi\n",
    "    return pi, val, count\n",
    "\n",
    "policy_iteration(mdp_transitions,1e-10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge\n",
    "def value_iteration(mdp, gamma=1.0, epsilon=1e-10):\n",
    "    val = {s: 0 for s in mdp}  # Initialize value function to zero for all states\n",
    "    count = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in mdp:\n",
    "            v = val[state]\n",
    "            q_values = []\n",
    "            for action in mdp[state]:\n",
    "                q_value = 0\n",
    "                for prob, next_state, reward, done in mdp[state][action]:\n",
    "                    q_value += prob * (reward + gamma * val[next_state])\n",
    "                q_values.append(q_value)\n",
    "            val[state] = max(q_values)\n",
    "            delta = max(delta, abs(v - val[state]))\n",
    "        count += 1\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "    pi = {state: max(mdp[state], key=lambda action: sum(prob * (reward + gamma * val[next_state])\n",
    "                                                        for prob, next_state, reward, done in mdp[state][action]))\n",
    "          for state in mdp}\n",
    "\n",
    "    return pi, val, count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to print the policy you got after running the policy iteration or value iteration on the 4x4 FrozenLake environment\n",
    "def print_policy(policy, env):\n",
    "    \"\"\"\n",
    "    Prints the policy for the 4x4 FrozenLake environment in a grid layout.\n",
    "    \"\"\"\n",
    "    action_symbols = {0: '←', 1: '↓', 2: '→', 3: '↑'}  #action symbols\n",
    "    grid_size = env.desc.shape  #get the grid dimensions (e.g., 4x4)\n",
    "    \n",
    "    policy_symbols = np.array([action_symbols[action] for cell,action in policy.items()])\n",
    "    policy_grid = policy_symbols.reshape(grid_size)  #reshape into a grid\n",
    "\n",
    "    print(\"Policy Grid:\")\n",
    "    for row in policy_grid:\n",
    "        print(\" \".join(row))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 3, 2: 3, 3: 3, 4: 0, 5: 0, 6: 0, 7: 0, 8: 3, 9: 1, 10: 0, 11: 0, 12: 0, 13: 2, 14: 1, 15: 0} ({0: 0.8235294094743778, 1: 0.8235294087066481, 2: 0.8235294081615119, 3: 0.823529407878649, 4: 0.8235294096410889, 5: 0, 6: 0.529411763068253, 7: 0, 8: 0.8235294099623763, 9: 0.8235294104148537, 10: 0.7647058811624488, 11: 0, 12: 0, 13: 0.8823529402179906, 14: 0.9411764700908947, 15: 0}, 1) 3\n",
      "{0: 0, 1: 3, 2: 3, 3: 3, 4: 0, 6: 0, 8: 3, 9: 1, 10: 0, 13: 2, 14: 1} {0: 0.8235294100518615, 1: 0.8235294094898987, 2: 0.8235294090993286, 3: 0.8235294089005962, 4: 0.8235294102241579, 5: 0, 6: 0.5294117635379374, 7: 0, 8: 0.8235294104939872, 9: 0.8235294108399842, 10: 0.7647058815425976, 11: 0, 12: 0, 13: 0.8823529405337791, 14: 0.9411764702612166, 15: 0} 591\n",
      "Policy Grid:\n",
      "← ↑ ↑ ↑\n",
      "← ← ← ←\n",
      "↑ ↓ ← ←\n",
      "← → ↓ ←\n"
     ]
    }
   ],
   "source": [
    "pi1, val1, count1 = policy_iteration(mdp_transitions,1e-10,1)\n",
    "pi2, val2, count2 = value_iteration(mdp_transitions,1,1e-10)\n",
    "\n",
    "print(pi1,val1,count1)\n",
    "print(pi2,val2,count2)\n",
    "\n",
    "print_policy(pi1,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write a function `test_policy()` to test your policy after training to find the number of times you reached the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(pi, env, goalstate):\n",
    "    # Complete this function to test the policy\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
